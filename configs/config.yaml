dataset_id: "ariG23498/license-detection-paligemma"
model_id: "google/gemma-3-4b-pt" # "unsloth/gemma-3-4b-it" 
checkpoint_id: "ajaymin28/Gemma3_ObjeDet"

device: "cuda"
dtype: "bfloat16"

batch_size: 16
learning_rate: 2e-5
epochs: 1
max_step_to_train: 100
validate_steps_freq: 10

finetune_method: "qlora"   # FFT | lora | qlora
use_unsloth: false


mm_tunable_parts:
  - no_exist_layer   # basically not finetuning any base components
  # - mlp
  # - multi_modal_projector
  # - vision_tower
  # - language_model
wandb_project_name:  "Gemma3_LoRA"
push_model_to_hub: true